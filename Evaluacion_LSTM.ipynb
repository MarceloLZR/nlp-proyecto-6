{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369d552c",
   "metadata": {},
   "source": [
    "# 📊 Evaluación rigurosa de modelos RNN: Perplejidad y palabras fuera de vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889768b",
   "metadata": {},
   "source": [
    "Este notebook presenta una implementación estructurada para evaluar modelos LSTM en tareas de modelado de lenguaje, \n",
    "haciendo especial énfasis en métricas como la *Perplejidad* (PP) y el manejo de palabras fuera de vocabulario (*Out-Of-Vocabulary*, OOV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3f22a",
   "metadata": {},
   "source": [
    "## 🔧 BLOQUE 1: Setup y descarga de datos\n",
    "\n",
    "- Instalación de dependencias necesarias (`torch`, `torchtext`, `nltk`, etc.).\n",
    "- Descarga del dataset **WikiText-2**.\n",
    "- Tokenización del corpus.\n",
    "- Construcción de vocabulario con diferentes tamaños:\n",
    "  - 10,000 palabras\n",
    "  - 30,000 palabras\n",
    "  - 50,000 palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc73ae0",
   "metadata": {},
   "source": [
    "# 🔧 Bloque 1: Setup y descarga de datos\n",
    "\n",
    "### 📚 Teoría\n",
    "El dataset **WikiText-2** es un corpus ampliamente utilizado para tareas de modelado de lenguaje. Contiene texto derivado de artículos de Wikipedia y es útil para entrenar y evaluar modelos de predicción de texto.\n",
    "\n",
    "Antes de entrenar cualquier modelo, es importante:\n",
    "1. Descargar el corpus.\n",
    "2. Tokenizar el texto.\n",
    "3. Construir un vocabulario de tamaño controlado (ej. 10^4, 3x10^4, 5x10^4 tokens).\n",
    "4. Dividir en datasets de entrenamiento, validación y prueba.\n",
    "\n",
    "Utilizaremos `torchtext`, que facilita todo este proceso con utilidades listas para usar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2059da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1)\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchtext) (4.66.5)\n",
      "Requirement already satisfied: requests in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchtext) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\marcelo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Downloading torchtext-0.18.0-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instalación de dependencias necesarias (si no están disponibles)\n",
    "!pip install torch torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787ec28f-f94d-43ba-8c11-7117fa9aa611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Downloading readme: 100%|██████████████████████████████████████████████████████████| 10.5k/10.5k [00:00<00:00, 7.24MB/s]\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████████| 733k/733k [00:00<00:00, 884kB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████| 6.36M/6.36M [00:00<00:00, 13.3MB/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████| 657k/657k [00:00<00:00, 2.08MB/s]\n",
      "Generating test split: 100%|█████████████████████████████████████████████| 4358/4358 [00:00<00:00, 113317.40 examples/s]\n",
      "Generating train split: 100%|██████████████████████████████████████████| 36718/36718 [00:00<00:00, 575961.91 examples/s]\n",
      "Generating validation split: 100%|███████████████████████████████████████| 3760/3760 [00:00<00:00, 460912.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloque 1 completado ✅\n"
     ]
    }
   ],
   "source": [
    "# ---- BLOQUE 1: Setup y descarga de datos ----\n",
    "\n",
    "# Instalación de librerías necesarias\n",
    "#!pip install datasets -q\n",
    "\n",
    "# Importar librerías\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "\n",
    "# Descargar recursos de NLTK (tokenizador)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Descargar WikiText-2 usando HuggingFace\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Tokenizar el texto\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# Preparar los datasets tokenizados\n",
    "train_tokens = [tokenize(example['text']) for example in dataset['train']]\n",
    "valid_tokens = [tokenize(example['text']) for example in dataset['validation']]\n",
    "test_tokens  = [tokenize(example['text']) for example in dataset['test']]\n",
    "\n",
    "# Construir vocabulario (ejemplo: 10k palabras más frecuentes)\n",
    "def build_vocab(token_lists, vocab_size=10000):\n",
    "    counter = Counter()\n",
    "    for tokens in token_lists:\n",
    "        counter.update(tokens)\n",
    "    vocab = build_vocab_from_iterator([counter.keys()], specials=[\"<unk>\"], max_tokens=vocab_size)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "# Crear tres vocabularios de diferentes tamaños\n",
    "vocab_10k = build_vocab(train_tokens, vocab_size=10_000)\n",
    "vocab_30k = build_vocab(train_tokens, vocab_size=30_000)\n",
    "vocab_50k = build_vocab(train_tokens, vocab_size=50_000)\n",
    "\n",
    "print(\"Bloque 1 completado ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d7faf-e9b7-44cb-98c4-af37877b92db",
   "metadata": {},
   "source": [
    "## 🧠 BLOQUE 2: Definición del modelo\n",
    "\n",
    "Se define un modelo LSTM con las siguientes características:\n",
    "\n",
    "- Embeddings de dimensión 300.\n",
    "- Dos capas LSTM con tamaño oculto de 512.\n",
    "- Capa final densa con softmax para predecir la siguiente palabra.\n",
    "\n",
    "**Componentes del modelo:**\n",
    "- `Embedding layer`\n",
    "- `LSTM layers (stacked)`\n",
    "- `Linear output layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0667b390-cb3f-4388-a2d8-cc647d4e1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BLOQUE 2: Definición del modelo ----\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=512, num_layers=2):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        \n",
    "        # Capa de embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # LSTM de 2 capas\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Capa final (proyección de hidden_dim a vocabulario)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        if hidden is None:\n",
    "            output, hidden = self.lstm(embeds)\n",
    "        else:\n",
    "            output, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3ed3c5-f030-435c-b706-1233ed7a03f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de salida logits: torch.Size([4, 20, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Crear un modelo de ejemplo\n",
    "vocab_size = len(vocab_10k)  # usamos el vocabulario de 10k palabras por ahora\n",
    "model = LSTMLanguageModel(vocab_size)\n",
    "\n",
    "# Probar con un batch falso (batch_size=4, secuencia de 20 tokens)\n",
    "x_dummy = torch.randint(0, vocab_size, (4, 20))  # random integers como tokens\n",
    "logits, hidden = model(x_dummy)\n",
    "\n",
    "print(f\"Shape de salida logits: {logits.shape}\")  # debería ser [4, 20, vocab_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218eb4d",
   "metadata": {},
   "source": [
    "## 🏋️ BLOQUE 3: Entrenamiento\n",
    "\n",
    "Entrenamos tres versiones del modelo, una por cada tamaño de vocabulario.\n",
    "\n",
    "**Funciones incluidas:**\n",
    "- Entrenamiento y validación por épocas.\n",
    "- Registro de métricas:\n",
    "  - *Perplejidad* sobre conjunto de validación.\n",
    "  - Conteo y porcentaje de palabras OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9c4c3d-7080-4128-bc67-86adc7686fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 1.2854 | Valid Loss: 1.1781 | Perplexity: 3.25\n",
      "Epoch 2/5 | Train Loss: 1.1382 | Valid Loss: 1.1497 | Perplexity: 3.16\n",
      "Epoch 3/5 | Train Loss: 1.1040 | Valid Loss: 1.1401 | Perplexity: 3.13\n",
      "Epoch 4/5 | Train Loss: 1.0834 | Valid Loss: 1.1389 | Perplexity: 3.12\n",
      "Epoch 5/5 | Train Loss: 1.0663 | Valid Loss: 1.1413 | Perplexity: 3.13\n"
     ]
    }
   ],
   "source": [
    "# ---- BLOQUE 3: Entrenamiento ----\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# 1. Dataset personalizado para lotes de secuencias\n",
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, token_lists, vocab, seq_len=30):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Convertir todo en una gran lista de IDs\n",
    "        self.data = [vocab[token] for tokens in token_lists for token in tokens if token.strip()]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        end = start + self.seq_len + 1\n",
    "        chunk = self.data[start:end]\n",
    "\n",
    "        # x son los primeros n tokens, y son los siguientes n tokens\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# 2. Funciones de entrenamiento y validación\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 3. Entrenamiento para un vocabulario (ejemplo vocab_10k)\n",
    "\n",
    "# Configuración\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "seq_len = 30\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Crear datasets y dataloaders\n",
    "train_dataset = LanguageModelDataset(train_tokens, vocab_10k, seq_len=seq_len)\n",
    "valid_dataset = LanguageModelDataset(valid_tokens, vocab_10k, seq_len=seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# Instanciar modelo\n",
    "model = LSTMLanguageModel(len(vocab_10k)).to(device)\n",
    "\n",
    "# Optimizador y función de pérdida\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, device)\n",
    "    perplexity = math.exp(valid_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f} | Perplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e029b5-109c-4205-8642-61a20711b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelo_10k.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195a75e4-6136-427d-991f-459043e1d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔢 Entrenando modelo con vocabulario de 10k palabras...\n",
      "\n",
      "[10k] Epoch 1/5 | Train: 1.2854 | Val: 1.1793 | Perplexity: 3.25\n",
      "[10k] Epoch 2/5 | Train: 1.1368 | Val: 1.1483 | Perplexity: 3.15\n",
      "[10k] Epoch 3/5 | Train: 1.1046 | Val: 1.1395 | Perplexity: 3.13\n",
      "[10k] Epoch 4/5 | Train: 1.0843 | Val: 1.1371 | Perplexity: 3.12\n",
      "[10k] Epoch 5/5 | Train: 1.0675 | Val: 1.1400 | Perplexity: 3.13\n",
      "\n",
      "🔢 Entrenando modelo con vocabulario de 30k palabras...\n",
      "\n",
      "[30k] Epoch 1/5 | Train: 2.1934 | Val: 1.8946 | Perplexity: 6.65\n",
      "[30k] Epoch 2/5 | Train: 1.9239 | Val: 1.8537 | Perplexity: 6.38\n",
      "[30k] Epoch 3/5 | Train: 1.8462 | Val: 1.8457 | Perplexity: 6.33\n",
      "[30k] Epoch 4/5 | Train: 1.7844 | Val: 1.8498 | Perplexity: 6.36\n",
      "[30k] Epoch 5/5 | Train: 1.7259 | Val: 1.8601 | Perplexity: 6.42\n",
      "\n",
      "🔢 Entrenando modelo con vocabulario de 50k palabras...\n",
      "\n",
      "[50k] Epoch 1/5 | Train: 3.9710 | Val: 3.5262 | Perplexity: 33.99\n",
      "[50k] Epoch 2/5 | Train: 3.5195 | Val: 3.4342 | Perplexity: 31.01\n",
      "[50k] Epoch 3/5 | Train: 3.3344 | Val: 3.4111 | Perplexity: 30.30\n",
      "[50k] Epoch 4/5 | Train: 3.1789 | Val: 3.4149 | Perplexity: 30.42\n",
      "[50k] Epoch 5/5 | Train: 3.0332 | Val: 3.4476 | Perplexity: 31.43\n"
     ]
    }
   ],
   "source": [
    "# ---- BLOQUE 3: Entrenamiento para los 3 tamaños de vocabulario ----\n",
    "\n",
    "# Vocabularios y etiquetas\n",
    "vocabularios = {\n",
    "    \"10k\": vocab_10k,\n",
    "    \"30k\": vocab_30k,\n",
    "    \"50k\": vocab_50k\n",
    "}\n",
    "\n",
    "# Hiperparámetros\n",
    "batch_size = 64\n",
    "seq_len = 30\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Resultados para comparar\n",
    "resultados = {}\n",
    "\n",
    "for nombre, vocab in vocabularios.items():\n",
    "    print(f\"\\n🔢 Entrenando modelo con vocabulario de {nombre} palabras...\\n\")\n",
    "\n",
    "    # Preparar datasets y dataloaders\n",
    "    train_dataset = LanguageModelDataset(train_tokens, vocab, seq_len=seq_len)\n",
    "    valid_dataset = LanguageModelDataset(valid_tokens, vocab, seq_len=seq_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Instanciar modelo, optimizador y loss\n",
    "    model = LSTMLanguageModel(len(vocab)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    historia = []\n",
    "\n",
    "    # Entrenamiento por épocas\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, device)\n",
    "        ppl = math.exp(valid_loss)\n",
    "\n",
    "        historia.append((train_loss, valid_loss, ppl))\n",
    "\n",
    "        print(f\"[{nombre}] Epoch {epoch+1}/{num_epochs} | Train: {train_loss:.4f} | Val: {valid_loss:.4f} | Perplexity: {ppl:.2f}\")\n",
    "\n",
    "    resultados[nombre] = historia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29b4e9ed-efd2-4c08-a3ec-17424e8c58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo con vocabulario 30k guardado como modelo_30k.pt\n"
     ]
    }
   ],
   "source": [
    "# 🔽 GUARDAR MODELO\n",
    "torch.save(model.state_dict(), f\"modelo_{nombre}.pt\")\n",
    "print(f\"✅ Modelo con vocabulario {nombre} guardado como modelo_{nombre}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28cebaec-c08b-4c7d-9b33-e63f874384de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo 50k guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"modelo_50k.pt\")\n",
    "print(\"✅ Modelo 50k guardado correctamente.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26eed21",
   "metadata": {},
   "source": [
    "\n",
    "## 🧩 BLOQUE 4: Manejo de OOV\n",
    "\n",
    "Se comparan tres estrategias de tratamiento para palabras fuera de vocabulario:\n",
    "\n",
    "1. **Token `<UNK>`:** se reemplazan todas las palabras desconocidas por un token especial.\n",
    "2. **Modelo char-level:** backoff a un modelo de caracteres para predecir embeddings.\n",
    "3. **Similitud de Levenshtein:** reemplazo por la palabra más cercana en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8f44b6-1517-4436-bccf-895bb733040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bee71c-0158-4782-b243-cfc7957b7f29",
   "metadata": {},
   "source": [
    "🧬 2. Backoff con modelo de nivel carácter (char-level LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9bd13b2-cdbd-4b91-b99b-ed4eb76b9a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, char_vocab_size, embed_dim=50, hidden_dim=128):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(char_vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, char_vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        output, _ = self.lstm(embeds)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b002dcae-efd9-43d5-9554-afb90d309682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.9/site-packages (0.27.1)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in /opt/conda/lib/python3.9/site-packages (from python-Levenshtein) (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/conda/lib/python3.9/site-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.13.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82585b96-8eb0-460b-872f-322cbafe92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def reemplazar_por_similitud(palabra, vocabulario):\n",
    "    vocab_set = set(vocabulario.get_itos())\n",
    "    if palabra in vocab_set:\n",
    "        return palabra\n",
    "\n",
    "    # Buscar palabra más similar\n",
    "    similar = min(vocab_set, key=lambda x: Levenshtein.distance(palabra, x))\n",
    "    return similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35f3c6b4-c474-4f78-9a3f-a00c053196b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Axis', '4s', '5', 'Apple', 'Axe', '48th', 'Con', 'Arras', \"'in\", 'Applying']\n"
     ]
    }
   ],
   "source": [
    "texto = \"This is a smaple txt with som errrs in spelling\"\n",
    "tokens_corr = [reemplazar_por_similitud(tok, vocab_10k) for tok in tokenize(texto)]\n",
    "print(tokens_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "357967ed-ae0e-4987-8428-9c3a24efbac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_corr = [reemplazar_por_similitud(tok, vocab) for tok in tokenize(texto)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080a96c",
   "metadata": {},
   "source": [
    "## 📈 BLOQUE 5: Evaluación\n",
    "\n",
    "Métricas medidas:\n",
    "\n",
    "- **Perplejidad** en el conjunto de prueba.\n",
    "- **Porcentaje de OOV** en test.\n",
    "- Comparación entre modelos con distintos vocabularios y estrategias OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67db73-1b33-4630-ad04-0485ca220779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔢 Evaluando modelo con vocabulario de 10k palabras...\n",
      "\n",
      "[10k] Perplejidad: 9922.71 | OOV Percentage: 77.92%\n",
      "\n",
      "🔢 Evaluando modelo con vocabulario de 30k palabras...\n",
      "\n",
      "[30k] Perplejidad: 29842.60 | OOV Percentage: 71.05%\n",
      "\n",
      "🔢 Evaluando modelo con vocabulario de 50k palabras...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Función para calcular la perplejidad\n",
    "def calcular_perplejidad(loss):\n",
    "    return math.exp(loss)\n",
    "\n",
    "# Función para calcular el porcentaje de OOV en el conjunto de test\n",
    "def calcular_oov_percentage(tokens_test, vocabulario):\n",
    "    oov_count = 0\n",
    "    total_words = 0\n",
    "    for tokens in tokens_test:\n",
    "        total_words += len(tokens)\n",
    "        oov_count += sum(1 for tok in tokens if tok not in vocabulario)\n",
    "    \n",
    "    oov_percentage = (oov_count / total_words) * 100\n",
    "    return oov_percentage\n",
    "\n",
    "# Evaluación para los 3 tamaños de vocabulario y las 3 estrategias OOV\n",
    "resultados_finales = {}\n",
    "\n",
    "for nombre, vocab in vocabularios.items():\n",
    "    print(f\"\\n🔢 Evaluando modelo con vocabulario de {nombre} palabras...\\n\")\n",
    "\n",
    "    # Preparar dataset de test\n",
    "    test_dataset = LanguageModelDataset(test_tokens, vocab, seq_len=seq_len)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Evaluar modelo\n",
    "    model = LSTMLanguageModel(len(vocab)).to(device)\n",
    "    model.load_state_dict(torch.load(f\"modelo_{nombre}.pt\"))  # Cargar el modelo entrenado\n",
    "\n",
    "    # Calcular la pérdida en el conjunto de test\n",
    "    test_loss = evaluate(model, test_loader, criterion, device)\n",
    "    perplejidad = calcular_perplejidad(test_loss)\n",
    "\n",
    "    # Calcular el porcentaje de OOV en el conjunto de test\n",
    "    oov_percentage = calcular_oov_percentage(test_tokens, vocab)\n",
    "\n",
    "    # Guardar resultados\n",
    "    \n",
    "    resultados_finales[nombre] = {\n",
    "        \"perplejidad\": perplejidad,\n",
    "        \"oov_percentage\": oov_percentage\n",
    "    }\n",
    "\n",
    "    print(f\"[{nombre}] Perplejidad: {perplejidad:.2f} | OOV Percentage: {oov_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73427393",
   "metadata": {},
   "source": [
    "## 📊 BLOQUE 6: Análisis y visualización\n",
    "\n",
    "Incluye:\n",
    "\n",
    "- Gráficos de perplejidad vs. tamaño de vocabulario.\n",
    "- Cobertura del vocabulario vs. porcentaje de OOV.\n",
    "- Comparativa de estrategias de manejo de OOV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558dc0eb",
   "metadata": {},
   "source": [
    "## 📘 BLOQUE 7: Reporte final\n",
    "\n",
    "Resumen de hallazgos:\n",
    "\n",
    "- Impacto del tamaño del vocabulario sobre la perplejidad.\n",
    "- Eficacia relativa de cada estrategia de OOV.\n",
    "- Recomendaciones para aplicaciones en producción.\n",
    "\n",
    "**Exportable a PDF o documento académico.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
